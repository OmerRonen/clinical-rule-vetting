{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit interpretable models to the training set and test on validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "from os.path import join as oj\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "import imodels\n",
    "from rulevetting.api import validation, util as api_util\n",
    "from rulevetting.projects.csi_pecarn.dataset import Dataset\n",
    "from rulevetting import DATA_PATH\n",
    "\n",
    "MODELS_DIR = './models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs {'clean_data': {'include_intervention': False, 'fillna': True}, 'preprocess_data': {'unclear_feat_default': 0, 'only_site_data': 2, 'augmented_features': True, 'use_control_type': 'all', 'fillna': True}, 'extract_features': {'drop_negative_columns': False}}\n"
     ]
    }
   ],
   "source": [
    "df_train, df_tune, df_test = Dataset().get_data()\n",
    "outcome_def = 'outcome'  # output\n",
    "meta_keys = api_util.get_feat_names_from_base_feats(df_train.columns, Dataset().get_meta_keys())\n",
    "\n",
    "# Keep age and site for transfer-trees project\n",
    "meta_keys.remove('AgeInYears')\n",
    "meta_keys.remove('SITE')\n",
    "\n",
    "X_train = df_train.drop(columns=meta_keys)\n",
    "X_tune = df_tune.drop(columns=meta_keys)\n",
    "X_test = df_test.drop(columns=meta_keys)\n",
    "\n",
    "pd.concat((X_train, X_tune, X_test)).astype(np.float32).to_csv(oj(DATA_PATH, 'imodels_data/csi_pred.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[outcome_def].values\n",
    "X_train = X_train.drop(columns=[outcome_def])\n",
    "y_tune = X_tune[outcome_def].values\n",
    "X_tune = X_tune.drop(columns=[outcome_def])\n",
    "y_test = X_test[outcome_def].values\n",
    "X_test = X_test.drop(columns=[outcome_def])\n",
    "processed_feats = X_train.keys().values.tolist()\n",
    "feature_names = processed_feats\n",
    "\n",
    "def predict_and_save(model, model_name='decision_tree'):\n",
    "    '''Plots cv and returns cv, saves all stats\n",
    "    '''\n",
    "    results = {'model': model}\n",
    "    for x, y, suffix in zip([X_train, X_tune],\n",
    "                            [y_train, y_tune],\n",
    "                            ['_train', '_tune']):\n",
    "        stats, threshes = validation.all_stats_curve(y, model.predict_proba(x.values)[:, 1],\n",
    "                                                     plot=suffix == '_tune')\n",
    "        for stat in stats.keys():\n",
    "            results[stat + suffix] = stats[stat]\n",
    "        results['threshes' + suffix] = threshes\n",
    "    pkl.dump(results, open(oj(MODELS_DIR, model_name + '.pkl'), 'wb'))\n",
    "    return stats, threshes\n",
    "\n",
    "def simple_report(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    print(\"Sensitivity: \", sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**decision tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit decition tree\n",
    "dt = DecisionTreeClassifier(max_depth=5, class_weight={0: 1, 1: 1000})\n",
    "dt.fit(X_train, y_train)\n",
    "stats, threshes = predict_and_save(dt, model_name='decision_tree')\n",
    "# plt.xlim((0.8, 1.0))\n",
    "# plt.ylim((0.5, 1.0))\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(50, 40))\n",
    "# plot_tree(dt, feature_names=feature_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 1000}\n",
    "grl = imodels.GreedyRuleListClassifier(max_depth=10, class_weight=class_weight, criterion='neg_corr')\n",
    "grl.fit(X_train, y_train, feature_names=feature_names)\n",
    "stats, threshes = predict_and_save(grl, model_name='grl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grl.rules_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rulefit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a rulefit model\n",
    "np.random.seed(13)\n",
    "rulefit = imodels.RuleFitClassifier(alpha=10, max_rules=None, random_state=0, tree_size=4, n_estimators=5, include_linear=True)\n",
    "rulefit.fit(X_train, y_train, feature_names=feature_names)\n",
    "\n",
    "# preds = rulefit.predict(X_test)\n",
    "stats, threshes = predict_and_save(rulefit, model_name='rulefit')\n",
    "'''\n",
    "def print_best(sens, spec):\n",
    "    idxs = np.array(sens) > 0.9\n",
    "    print(np.array(sens)[idxs], np.array(spec)[idxs])\n",
    "print_best(sens, spec)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.reset_option('display.max_colwidth')\n",
    "rulefit.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulefit.complexity_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original CDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_cdr_predict(X, use_2 = False):\n",
    "    num_conditions = (X['AlteredMentalStatus2'] + \n",
    "                      X['FocalNeuroFindings2'] + \n",
    "                      X['PainNeck2'] +\n",
    "                      X['Torticollis2'] + \n",
    "                      X['subinj_TorsoTrunk2'] + \n",
    "                      X['Predisposed'] + \n",
    "                      X['HighriskDiving'] + \n",
    "                      X['HighriskMVC'])\n",
    "    preds = (num_conditions > 0).astype(int).values\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_train = pd.concat((X_train, X_tune))\n",
    "y_all_train = np.concatenate((y_train, y_tune), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_report(y_all_train, baseline_cdr_predict(X_all_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_report(y_train, baseline_cdr_predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_report(y_tune, baseline_cdr_predict(X_tune))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_report(y_test, baseline_cdr_predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skope = imodels.SkopeRulesClassifier(precision_min=0.01, recall_min=0.1, n_estimators=10, max_samples=0.8, bootstrap=True, max_depth=3, random_state=0)\n",
    "skope.fit(X_train, y_train)\n",
    "stats, threshes = predict_and_save(skope, model_name='skope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skope.rules_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = imodels.BoostedRulesClassifier(n_estimators=10,)\n",
    "boost.fit(X_train, y_train)\n",
    "stats, threshes = predict_and_save(boost, model_name='boostedrules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(r, w) for r, w in boost.rules_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saps = imodels.SaplingSumClassifier(max_rules=20)\n",
    "saps.fit(X_train.values, y_train, feature_names=feature_names)\n",
    "stats, threshes = predict_and_save(saps, model_name='saps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['spec'][1], stats['sens'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(100, ))\n",
    "nn.fit(X_train, y_train)\n",
    "stats, threshes = predict_and_save(nn, model_name='nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CORELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corels = imodels.OptimalRuleListClassifier(\n",
    "#     c=0.0000001, n_iter=100000, map_type=\"prefix\", policy=\"dfs\", verbosity=[], ablation=0, max_card=2, min_support=0.01, random_state=5)\n",
    "# corels.fit(X_train, y_train, feature_names=feature_names)\n",
    "# stats, threshes = predict_and_save(corels, model_name='corels')\n",
    "# print(corels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stablerules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imodels.experimental import stablelinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_learners = [rulefit, skope, boost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stbl = stablelinear.StableLinearClassifier(weak_learners=weak_learners, max_complexity=-1, alpha=0.1, max_rules=None, penalty='l1')\n",
    "stbl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, threshes = predict_and_save(stbl, model_name='stbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stbl.rules_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['sens'][1], stats['spec'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(suffix, title=None, fs=15):\n",
    "    for fname in sorted(os.listdir(MODELS_DIR)):\n",
    "        if 'pkl' in fname:\n",
    "            if not fname[:-4] == 'rf':\n",
    "                r = pkl.load(open(oj(MODELS_DIR, fname), 'rb'))\n",
    "                #         print(r)\n",
    "                #                 print(r.keys())\n",
    "\n",
    "                threshes = np.array(r['threshes' + suffix])\n",
    "                sens = np.array(r['sens' + suffix])\n",
    "                spec = np.array(r['spec' + suffix])\n",
    "                plt.plot(100 * sens, 100 * spec, 'o-', label=fname[:-4], alpha=0.6, markersize=3)\n",
    "                plt.xlabel('Sensitivity (%)', fontsize=fs)\n",
    "                plt.ylabel('Specificity (%)', fontsize=fs)\n",
    "                s = suffix[1:]\n",
    "                if title is None:\n",
    "                    plt.title(f'{s}\\n{data_sizes[s][0]} IAI-I / {data_sizes[s][1]}')\n",
    "                else:\n",
    "                    plt.title(title, fontsize=fs)\n",
    "\n",
    "                # print best results\n",
    "                if suffix == '_test2':\n",
    "                    idxs = (sens > 0.95) & (spec > 0.43)\n",
    "                    if np.sum(idxs) > 0:\n",
    "                        idx_max = np.argmax(spec[idxs])\n",
    "                        print(fname, f'{100 * sens[idxs][idx_max]:0.2f} {100 * spec[idxs][idx_max]:0.2f}')\n",
    "\n",
    "    if suffix == '_train':\n",
    "        plt.plot(97.79, 31.05, 'o', color='black', label='Original CDR', ms=4)\n",
    "    if suffix == '_tune':\n",
    "        plt.plot(96.19, 31.78, 'o', color='black', label='Original CDR', ms=4)\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "suffixes = ['_train', '_tune']  # _train, _test1, _test2, _cv\n",
    "titles = ['Train (CSI PECARN)', 'Tune (CSI PECARN)']\n",
    "R, C = 1, len(suffixes)\n",
    "plt.figure(dpi=200, figsize=(C * 2.5, R * 3), facecolor='w')\n",
    "fs = 10\n",
    "for i, suffix in enumerate(suffixes):\n",
    "    ax = plt.subplot(R, C, i + 1)\n",
    "    plot_metrics(suffix, title=titles[i], fs=fs)\n",
    "    if i > 0:\n",
    "        plt.ylabel('')\n",
    "        plt.yticks([0, 25, 50, 75, 100], labels=[''] * 5)\n",
    "    #         ax.yaxis.set_visible(False)\n",
    "    plt.xlim((50, 101))\n",
    "    plt.ylim((0, 101))\n",
    "plt.tight_layout()\n",
    "# plt.subplot(R, C, 1)\n",
    "# plt.legend(fontsize=20)\n",
    "plt.legend(bbox_to_anchor=(1.1, 1), fontsize=fs, frameon=False)\n",
    "#plt.savefig('figs/metrics_3_splits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
